{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.11 - Pima Indians Diabetes - Adaboost and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import pandas as pd \n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "df = pd.read_csv('pima-indians-diabetes.csv')\n",
    "\n",
    "# extract the values from the columns in a form of an array\n",
    "array = df.values\n",
    "X = array[:, 0:8]\n",
    "y = array[:, 8]\n",
    "\n",
    "# set the random seed as 7 and number of trees as 30\n",
    "seed = 7   \n",
    "num_trees = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build classifiers using adaboost and XGBoot.\n",
    "\n",
    "Let's create the adaboost model using the scikitlearn. \n",
    "\n",
    "__Adaboost uses decision tree classifier as the default classifier.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujuay\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the model by splitting the train test indexs into 10 consecutive folds. \n",
    "\n",
    "Pass the model within the cross validation model function to evaluate the result using the cross validation techinque\n",
    "\n",
    "Again evaluate the model such that each fold gets used once as a validation while the remaining 9 folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7525974025974026\n"
     ]
    }
   ],
   "source": [
    "results = model_selection.cross_val_score(model, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost gives an accuracy of 76%. Similarly, we apply the __XGBoost algorithm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using XGBoost to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# initialize the XGBClassifier under the name clf and set the seed value as y and number of trees as 30\n",
    "clf = XGBClassifier()\n",
    "seed = 7\n",
    "num_trees=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct the xgboost classifiser using the k-fold technique such that number of folds equals 10 and evaluate the model using the cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7668660287081339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujuay\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = XGBClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the performance of Adaboost using 30 trees was better than XGBoosts performance. I adjusted the number of trees to make XGBoost better for demonstration purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
