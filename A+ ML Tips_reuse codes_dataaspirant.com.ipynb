{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "### Data Imputation:\n",
    "\n",
    "Imputation is a process of replacing missing values with substituted values. In our dataset, some columns have missing values. We can replace missing values with mean, median, mode or any particular value.\n",
    "Sklearn provides Imputer() method to perform imputation in 1 line of code. We just need to define missing_values, axis, and strategy. We are using “median” value of the column to substitute with the missing value.\n",
    "\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values=\"NaN\", strategy='median', axis=0)\n",
    "    X = imp.fit_transform(X)\n",
    "    \n",
    "If you specify strategy as 'constant', you'll need to specify the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test = imp.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_predict = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection\n",
    "\n",
    "correlation with dependent variable\n",
    "\n",
    "    cor = train_df.corr()\n",
    "    cor_target = abs(cor['y'])\n",
    "\n",
    "selecting highly correlated features\n",
    "\n",
    "    relevant_features = cor_target[cor_target>0.5]\n",
    "    relevant_features\n",
    "    \n",
    "Recall that for linear regression, __multicolinearity__ is a big issue so you need to also check the correlations among the features and if 2 features are correlated, you drop one. If you have 1 feature correlating with more than 1 features, that will be the best feature to drop.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = train_df.corr()\n",
    "cor_target = abs(cor['y'])\n",
    "\n",
    "#selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score\n",
    "\n",
    "__accuracy_score()__: This function is used to print accuracy of KNN algorithm. By accuracy, we mean the ratio of the correctly predicted data points to all the predicted data points. Accuracy as a metric helps to understand the effectiveness of our algorithm. It takes 4 parameters.\n",
    "\n",
    "- y_true\n",
    "- y_pred\n",
    "- normalize\n",
    "- sample_weight\n",
    "\n",
    "Out of these 4, normalize & sample_weight are optional parameters. The parameter y_true  accepts an array of correct labels and y_pred takes an array of predicted labels that are returned by the classifier. It returns accuracy as a float value.\n",
    "\n",
    "\n",
    "    # to check the accuracy score\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the accuracy score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Testing data accuracy score')\n",
    "print(metrics.accuracy_score(y_test, y_pred)\n",
    "print('=============================')\n",
    "print('Training data accuracy score')\n",
    "print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "In addition to the accuracy_score, the confusion matrix can also be used to check the accuracy of the model prediction.\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # print the classification report\n",
    "    print(classification_report((y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report((y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using KNN modeling example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for K in range(15):  # generate a range of numbers for k i.e. from 1 - 15\n",
    "    K_value = K+1\n",
    "    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n",
    "    neigh.fit(X_train, y_train) \n",
    "    y_pred = neigh.predict(X_test)\n",
    "    print \"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\",K_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K value Vs Accuracy Change Graph\n",
    "\n",
    "Plot the Accuracy on y axis and K-value on x axis to see the optimum accuracy\n",
    "\n",
    "Fig: K-value Vs Accuracy\n",
    "\n",
    "It shows that we are getting 95.71% accuracy on K = 3, 5. Choosing a large value of K will lead to greater amount of execution time & underfitting. Selecting the small value of K will lead to overfitting. There is no such guaranteed way to find the best value of K. So, to run it quickly we are considering K =3 for this tutorial.\n",
    "\n",
    "__Note__: Implementing KNN on cross-validation data. It helps to validate which K value may give the better accuracy.\n",
    "\n",
    "A very low K value may be overfitted while a high k value defeats the purpose as boundaries will be crossed, thereby reducing the accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib - Fit curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# example data\n",
    "mu = 100 # mean of distribution\n",
    "sigma = 15 # standard deviation of distribution\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    " \n",
    "num_bins = 50\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, num_bins, density=1, facecolor='green', alpha=0.5)\n",
    "# add a 'best fit' line\n",
    "y = norm.pdf(bins, mu, sigma)\n",
    "plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')\n",
    " \n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "https://dataaspirant.wordpress.com/2014/10/02/linear-regression/\n",
    "\n",
    "Linear Regression means predicting scores of one variable from the scores of second variable. The variable we are predicting is called the __criterion/depenndent/output variable__ and is referred to as Y. The variable we are basing our predictions is called the __predictor/independent/input variable__ and is referred to as X. \n",
    "\n",
    "When there is only one predictor variable, the prediction method is called __simple regression__.\n",
    "\n",
    "The aim of linear regression is to finding the best-fitting straight line through the points. The best-fitting line is called a __regression line.__\n",
    "\n",
    "       hθ(x) = θ0 + θ1X\n",
    "The above equation is __hypothesis equation__\n",
    "\n",
    "where:\n",
    "\n",
    "hθ(x) is nothing but the value Y(*which we are going to predicate* )  for particular x ( means Y is a linear function of x)\n",
    "\n",
    "θ0 is a constant equal to the y intercept of the regression line\n",
    "\n",
    "θ1 is  the __regression coefficient__ i.e.  the average change in the dependent variable (Y) for a 1-unit change in the independent variable (X). It is the slope of the regression line\n",
    "\n",
    "X is value of the independent variable\n",
    "\n",
    "### Properties of the Linear Regression Line\n",
    "Linear Regression line has the following properties:\n",
    "\n",
    "1. The line minimizes the sum of squared differences between observed values (the y values) and predicted values (the hθ(x) values computed from the regression equation).\n",
    "2. The regression line passes through the mean of the X values (x) and through the mean of the Y values ( hθ(x) ).\n",
    "3. The regression constant (θ0) is equal to the y intercept of the regression line.\n",
    "4. The regression coefficient (θ1) is the average change in the dependent variable (Y) for a 1-unit change in the independent variable (X). It is the slope of the regression line.\n",
    "\n",
    "#### The least squares regression line is the only straight line that has all of these properties.\n",
    "\n",
    "\n",
    "### Goal of  Hypothesis Function\n",
    "Goal of Hypothesis is to choose θ0 and θ1 , so that hθ(x) is close to Y for our training data,while choosing θ0 and θ1 we have to consider the cost function( J(θ) ) where we are getting low value for cost function( J(θ) ).\n",
    "\n",
    "The below function is called as cost function, cost function ( J(θ) ) is nothing but just a __Squared error function.__\n",
    "\n",
    "\n",
    "### Strategy\n",
    "- Start with some θ0, θ1\n",
    "- Keepchanging θ0, θ1 to reduce J(θ0,θ1) until we hopefully end up at a minimum\n",
    "\n",
    "https://dataaspirant.com/linear-regression-implementation-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show the resutls of linear fit model\n",
    "def show_linear_line(X_parameters,Y_parameters):\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_parameters, Y_parameters)\n",
    "    plt.scatter(X_parameters,Y_parameters,color='blue')\n",
    "    plt.plot(X_parameters,regr.predict(X_parameters),color='red',linewidth=4)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()   \n",
    "    \n",
    "\n",
    "# to call the show linear line function\n",
    "show_linear_line(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Vs Clustering\n",
    "\n",
    "https://dataaspirant.com/classification-clustering-alogrithms/\n",
    "\n",
    "https://sites.google.com/site/dataclusteringalgorithms/home\n",
    "\n",
    "## Classification Concept\n",
    "In classification, the idea is to predict the target class by analysis the training dataset. This could be done by finding proper boundaries for each target class. In a general way of saying, Use the training dataset to get better boundary conditions which could be used to determine each target class. Once the boundary conditions determined, the next task is to predict the target class as we have said earlier. The whole process is known as classification.\n",
    "\n",
    "### Classification Algorithms\n",
    "- Linear classifiers\n",
    "    - Logistic regression\n",
    "    - Naive Bayes classifier\n",
    "    - Fisher’s linear discriminant\n",
    "- Support vector machines\n",
    "    - Least squares support vector machines\n",
    "- Quadratic classifiers\n",
    "- Kernel estimation\n",
    "    - k-nearest neighbor\n",
    "- Decision trees\n",
    "    - Random forests\n",
    "- Neural networks\n",
    "- Learning vector quantization\n",
    "\n",
    "### Application of Classification Algorithms\n",
    "- Email spam classification\n",
    "- Bank customers loan pay bank willingness prediction.\n",
    "- Cancer tumour cells identification.\n",
    "- Sentiment analysis.\n",
    "- Drugs classification\n",
    "- Facial key points detection\n",
    "- Pedestrians detection in an automotive car driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Concept\n",
    "\n",
    "In clustering the idea is not to predict the target class as like classification , it’s more ever trying to group the similar kind of things by considering the most satisfied condition __all the items in the same group should be similar and no two different group items should not be similar__.  To group the similar kind of items in clustering, different similarity measures could be used i.e similarity measure is used to cluster.\n",
    "\n",
    "### Group items Examples:\n",
    "While grouping similar language type documents (Same language documents are one group.)\n",
    "While categorising the news articles (Same news category(Sport) articles are one group )\n",
    "\n",
    "\n",
    "## Clustering Algorithms\n",
    "Clustering algorithms can be classified into two main categories \n",
    "- Linear clustering algorithms and \n",
    "- Non-linear clustering algorithms.\n",
    "\n",
    "### Linear clustering algorithm\n",
    "- k-means clustering algorithm\n",
    "- Fuzzy c-means clustering algorithm\n",
    "- Hierarchical clustering algorithm\n",
    "- Gaussian(EM) clustering algorithm\n",
    "- Quality threshold clustering algorithm\n",
    "\n",
    "### Non-linear clustering algorithm\n",
    "- MST based clustering algorithm\n",
    "- kernel k-means clustering algorithm\n",
    "- Density-based clustering algorithm\n",
    "\n",
    "### Application of Clustering Algorithms\n",
    "- Recommender systems\n",
    "- Anomaly detection\n",
    "- Human genetic clustering\n",
    "- Genom Sequence analysis\n",
    "- Analysis of antimicrobial activity\n",
    "- Grouping of shopping items\n",
    "- Search result grouping\n",
    "- Slippy map optimization\n",
    "- Crime analysis\n",
    "- Climatology\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "__Classification__: Predicting target class for test dataset from the trained modeled from the training dataset.\n",
    "\n",
    "__Clustering__: Using different similarity measure to place the all the similar items in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Concept\n",
    "\n",
    "https://dataaspirant.com/how-logistic-regression-model-works/\n",
    "\n",
    "“Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities __(likelihood occurrence)__ using a __logistic function__” (Wikipedia)\n",
    "\n",
    "It uses a black box function to understand the relation between the categorical __dependent variable__ and the __independent variables__. This black box function is popularly known as the __Softmax funciton__.\n",
    "\n",
    "#### Two functions which determine the logistic regression model. Just for a glance:\n",
    "\n",
    "- __Softmax__: Used for the multi-classification task.\n",
    "     - The probabilities sum will be 1\n",
    "     - Used in the different layers of neural networks.\n",
    "     - The high value will have the higher probability than other values.\n",
    "- __Sigmoid__: Used for the binary classification task.\n",
    "     - The probabilities sum need not be 1.\n",
    "     - Used as activation function while building neural networks.\n",
    "     - The high value will have the high probability but not the higher probability.\n",
    "\n",
    "https://dataaspirant.com/difference-between-softmax-function-and-sigmoid-function/\n",
    "\n",
    "### Examples of likelihood occurrence of an event\n",
    "- How likely a customer will buy iPod having iPhone in his/her pocket.\n",
    "- How likely Argentina team will win when Lionel Andrés Messi in rest.\n",
    "- What is the probability to get into best university by scoring decent marks in mathematics, physics?\n",
    "- What is the probability to get a kiss from your girlfriend when you gifted her favorite dress on behalf of your birthday?\n",
    "\n",
    "If the logistic regression model used for addressing the binary classification kind of problems it’s known as the __binary logistic regression classifier__. Whereas the logistic regression model used for multiclassification kind of problems, it’s called the __multinomial logistic regression classifier__ i.e. using the logistic regression techniques to predict target with __more than 2 target classes__.\n",
    "\n",
    "The underline technique will be same like the logistic regression for binary classification until calculating the probabilities for each target. Once the probabilities were calculated. We need to transfer them into __one hot encoding__ and uses the cross entropy methods in the training process for calculating the properly optimized weights.\n",
    "\n",
    "Multinomial logistic regression works well on big data irrespective of different areas. Surprisingly it is also used in human resource development and more in depth details about how the big data is used in human resource development can found in this article.\n",
    "\n",
    "https://dataaspirant.com/multinomial-logistic-regression-model-works-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://dataaspirant.com/implement-logistic-regression-model-python-binary-classification/\n",
    "    \n",
    "# About: Implementing Logistic Regression Classifier to predict to whom the voter will vote.\n",
    " \n",
    "# Required Python Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    " \n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "py.sign_in('dataaspirant', 'RhJdlA1OsXsTjcRA0Kka')\n",
    " \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    " \n",
    "    \n",
    "# file \n",
    "DATA_SET_PATH = \"../Inputs/anes_dataset.csv\"\n",
    " \n",
    " \n",
    "def dataset_headers(dataset):\n",
    "        \"\"\"\n",
    "    To get the dataset header names\n",
    "    :param dataset: loaded dataset into pandas DataFrame\n",
    "    :return: list of header names\n",
    "    \"\"\"\n",
    "    return list(dataset.columns.values)\n",
    " \n",
    " \n",
    "def unique_observations(dataset, header, method=1):\n",
    "    \"\"\"\n",
    "    To get unique observations in the loaded pandas DataFrame column\n",
    "    :param dataset:\n",
    "    :param header:\n",
    "    :param method: Method to perform the unique (default method=1 for pandas and method=0 for numpy )\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method == 0:\n",
    "            # With Numpy\n",
    "            observations = np.unique(dataset[[header]])\n",
    "        elif method == 1:\n",
    "            # With Pandas\n",
    "            observations = pd.unique(dataset[header].values.ravel())\n",
    "        else:\n",
    "            observations = None\n",
    "            print \"Wrong method type, Use 1 for pandas and 0 for numpy\"\n",
    "    except Exception as e:\n",
    "        observations = None\n",
    "        print \"Error: {error_msg} /n Please check the inputs once..!\".format(error_msg=e.message)\n",
    "    return observations\n",
    " \n",
    " \n",
    "def feature_target_frequency_relation(dataset, f_t_headers):\n",
    " \n",
    "    \"\"\"\n",
    "    To get the frequency relation between targets and the unique feature observations\n",
    "    :param dataset:\n",
    "    :param f_t_headers: feature and target header\n",
    "    :return: feature unique observations dictionary of frequency count dictionary\n",
    "    \"\"\"\n",
    " \n",
    "    feature_unique_observations = unique_observations(dataset, f_t_headers[0])\n",
    "    unique_targets = unique_observations(dataset, f_t_headers[1])\n",
    " \n",
    "    frequencies = {}\n",
    "    for feature in feature_unique_observations:\n",
    "        frequencies[feature] = {unique_targets[0]: len(\n",
    "            dataset[(dataset[f_t_headers[0]] == feature) & (dataset[f_t_headers[1]] == unique_targets[0])]),\n",
    "            unique_targets[1]: len(\n",
    "                dataset[(dataset[f_t_headers[0]] == feature) & (dataset[f_t_headers[1]] == unique_targets[1])])}\n",
    "    return frequencies\n",
    " \n",
    " \n",
    "def feature_target_histogram(feature_target_frequencies, feature_header):\n",
    "    \"\"\"\n",
    " \n",
    "    :param feature_target_frequencies:\n",
    "    :param feature_header:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    keys = feature_target_frequencies.keys()\n",
    "    y0 = [feature_target_frequencies[key][0] for key in keys]\n",
    "    y1 = [feature_target_frequencies[key][1] for key in keys]\n",
    " \n",
    "    trace1 = go.Bar(\n",
    "        x=keys,\n",
    "        y=y0,\n",
    "        name='Clinton'\n",
    "    )\n",
    "    trace2 = go.Bar(\n",
    "        x=keys,\n",
    "        y=y1,\n",
    "        name='Dole'\n",
    "    )\n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(\n",
    "        barmode='group',\n",
    "        title='Feature :: ' + feature_header + ' Clinton Vs Dole votes Frequency',\n",
    "        xaxis=dict(title=\"Feature :: \" + feature_header + \" classes\"),\n",
    "        yaxis=dict(title=\"Votes Frequency\")\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    # plot_url = py.plot(fig, filename=feature_header + ' - Target - Histogram')\n",
    "    py.image.save_as(fig, filename=feature_header + '_Target_Histogram.png')\n",
    " \n",
    " \n",
    "def train_logistic_regression(train_x, train_y):\n",
    "    \"\"\"\n",
    "    Training logistic regression model with train dataset features(train_x) and target(train_y)\n",
    "    :param train_x:\n",
    "    :param train_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    " \n",
    "    logistic_regression_model = LogisticRegression()\n",
    "    logistic_regression_model.fit(train_x, train_y)\n",
    "    return logistic_regression_model\n",
    " \n",
    " \n",
    "def model_accuracy(trained_model, features, targets):\n",
    "    \"\"\"\n",
    "    Get the accuracy score of the model\n",
    "    :param trained_model:\n",
    "    :param features:\n",
    "    :param targets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    accuracy_score = trained_model.score(features, targets)\n",
    "    return accuracy_score\n",
    " \n",
    " \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier main\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the data set for training and testing the logistic regression classifier\n",
    "    dataset = pd.read_csv(DATA_SET_PATH)\n",
    "    print \"Number of Observations :: \", len(dataset)\n",
    " \n",
    "    # Get the first observation\n",
    "    print dataset.head()\n",
    " \n",
    "    headers = dataset_headers(dataset)\n",
    "    print \"Data set headers :: {headers}\".format(headers=headers)\n",
    " \n",
    "    training_features = ['TVnews', 'PID', 'age', 'educ', 'income']\n",
    "    target = 'vote'\n",
    " \n",
    "    # Train , Test data split\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[training_features], dataset[target], train_size=0.7)\n",
    "    print \"train_x size :: \", train_x.shape\n",
    "    print \"train_y size :: \", train_y.shape\n",
    " \n",
    "    print \"test_x size :: \", test_x.shape\n",
    "    print \"test_y size :: \", test_y.shape\n",
    " \n",
    "    print \"edu_target_frequencies :: \", feature_target_frequency_relation(dataset, [training_features[3], target])\n",
    " \n",
    "    for feature in training_features:\n",
    "        feature_target_frequencies = feature_target_frequency_relation(dataset, [feature, target])\n",
    "        feature_target_histogram(feature_target_frequencies, feature)\n",
    " \n",
    "    # Training Logistic regression model\n",
    "    trained_logistic_regression_model = train_logistic_regression(train_x, train_y)\n",
    "    \n",
    "    train_accuracy = model_accuracy(trained_logistic_regression_model, train_x, train_y)\n",
    " \n",
    "    # Testing the logistic regression model\n",
    "    test_accuracy = model_accuracy(trained_logistic_regression_model, test_x, test_y)\n",
    " \n",
    "    print \"Train Accuracy :: \", train_accuracy\n",
    "    print \"Test Accuracy :: \", test_accuracy\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x200b6379e08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required Packages\n",
    " \n",
    "from datetime import datetime\n",
    " \n",
    "x = [ \n",
    "datetime(year=2000,month=1,day=1),\n",
    "datetime(year=2001,month=1,day=1),\n",
    "datetime(year=2002,month=1,day=1),\n",
    "datetime(year=2003,month=1,day=1),\n",
    "datetime(year=2004,month=1,day=1),\n",
    "datetime(year=2005,month=1,day=1),\n",
    "datetime(year=2006,month=1,day=1),\n",
    "datetime(year=2007,month=1,day=1),\n",
    "datetime(year=2008,month=1,day=1),\n",
    "datetime(year=2009,month=1,day=1),\n",
    "datetime(year=2010,month=1,day=1),\n",
    "datetime(year=2011,month=1,day=1),\n",
    "datetime(year=2012,month=1,day=1)\n",
    "]\n",
    " \n",
    "y = [1014004000, 1029991000, 1045845000, 1049700000, 1065071000, 1080264000, 1095352000, 1129866000, 1147996000, 1166079000, 1173108000, 1189173000, 1205074000]\n",
    "\n",
    "plt.scatter(x, y, c='yellow', linewidths=0.5, edgecolors='black')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the target variable for Bias\n",
    "\n",
    "If the target/dependent variable is __categorical__ i.e not numeric, you need to check that the values for the categorical data is not biased i.e. you don't have one label significantly more in number than the other(s).\n",
    "\n",
    "Method 1:\n",
    "    \n",
    "    print('Total number of labels: {}'.format(df.shape[0]))\n",
    "    print('Number of male: {}'.format(df[df.columnname == 'male'].shape[0]))\n",
    "    print('Number of female: {}'.format(df[df[columnname] == 'female'].shape[0]))\n",
    "\n",
    "where columnname refers to the dependent variable/column name. compare the numbers\n",
    "\n",
    "Method 2:   \n",
    "\n",
    "    df['columname'].value_counts().plot.bar()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Features\n",
    "\n",
    "Standardizing your features is part of EDA. It's used to scale the data between -1 and 1 so once you go through your data and see that some features are say in the thousands while others are in the hundreds or even less than 1, you'll need to normalize/standardize the data after splitting X and y.\n",
    "\n",
    "First Split X and y\n",
    "\n",
    "    X = df.drop('target_column', axis=1)\n",
    "    y = df['target_column']\n",
    "\n",
    "### label encode target column (y) if it's not numeric \n",
    "i.e. to convert it from label to numeric as ML only use numeric\n",
    "\n",
    "     from sklearn.preprocessing import LabelEncoder \n",
    "     label_encoder = LabelEncoder()  # initialize\n",
    "     y = label_encoder.fit_transform(y)\n",
    "     # y will become an ndarray\n",
    "     type(y)\n",
    "     \n",
    "### Standardize the features\n",
    "scale the data to be between -1 and 1\n",
    "\n",
    "      from sklearn.preprocessing import StandardScaler\n",
    "      scaler = StandardScaler()\n",
    "      scaler.fit(X)\n",
    "      X = scaler.transform(X)\n",
    "      \n",
    "Split your data into train and test and carry on with initializing the algorithm, fitting the data (model), make predictions using the test datav(X_test) and then another with the train data (X_train), and comparing accuracy of test (y_test, y_test_pred) and train data (y_train, y_train_pred) to check for overfitting. See example code using Support Vector Machines below:\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()  # initilize the support vector classifier\n",
    "    svc_model.fit(X_train, y_train)  # model\n",
    "    y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "    \n",
    "    y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "    \n",
    "    # check the accuracy\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Encoding is required because Machine Learning only deals with numbers so categorical data/labels cannot be used in our models without converting it to numbers first.\n",
    "\n",
    "## One Hot Encoding\n",
    "\n",
    "One hot encoding is used for encoding features and will create new columns for the classes within the column and assign a 1 for a value under its column and 0 to other columns.\n",
    "\n",
    "    pd.get_dummies(dataframe['column_name'])\n",
    "    \n",
    "To perform one hot encoding, create a list with the list of all the columns to encode and pass it where you have 'column_name' in above formula.\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "Label encoding is used for the target variable because it does not create new additional columns like one-hot-encoding. We cannot have additional columns for our target/dependent variable. The label usually starts from 0.\n",
    "\n",
    "     from sklearn.preprocessing import LabelEncoder \n",
    "     label_encoder = LabelEncoder()  # initialize\n",
    "     y = label_encoder.fit_transform(dataframe['column_name'])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "#### Numerical features need to be converted into categorical data inorder to use decision tree \n",
    "\n",
    "#### Decision Tree Algorithm Advantages and Disadvantages\n",
    "__Pros:__\n",
    "- simple to understand, interpret, visualize\n",
    "- It performed variable screening or feature selection\n",
    "- It can handle both numerica and categorical data (CART)\n",
    "- It can handle multi output problems\n",
    "- requires little effor for data preparation\n",
    "- nonlinear relationship between parameters do not affect the performance\n",
    "\n",
    "__Cons:__\n",
    "- can create over complex trees that do not generalize the data well - overfitting.\n",
    "- can become unstable because small variations in data can result in a completely different tree being generated. This is called __variance__ and needs to be lowered by methods of __bagging__ and __boosting__.\n",
    "- greedy algorithms cannot be guaranteed to return the globally optimal decision tree. This can be mitigated by training multiple trees where features and samples are randomly selected with replacement.\n",
    "- Decision tree learners also create bias trees if some classes dominate it. It is therefore recommended to balance the data set prior to fitting with decision tree.\n",
    "\n",
    "__What is a greedy algorithm__\n",
    "As the name suggests, a greedy algorithm always makes the choice that seems to be the best at the moment. This means that it makes a locally-optimal choice in the hope that this choice will lead to a globally-optimal solution.\n",
    "\n",
    "\n",
    "#### Applications\n",
    "1. Customer will pay his insurance premium\n",
    "2. If male/female on the titanic, what are the chances of survival\n",
    "3. If a person is male/female based on height and wieight\n",
    "4. Price of a house based on the number of rooms as well as the floor size\n",
    "\n",
    "Decision tree is drawn like an upside down tree\n",
    "\n",
    "\n",
    "#### Differences and Similarities between Classification and Regression Trees (CART)\n",
    "\n",
    "Regression trees are used when the dependent variable is continuous.\n",
    "Classification trees are used when the dependent variable is categorical.\n",
    "\n",
    "In the case of regression trees, the value obtained by terminal nodes is the mean or average response of the observation falling in that region thus if an unseen data observation falls in that region will make it's prediction with a mean value.\n",
    "\n",
    "In the cases of classification trees, the value or class obtained is the mode (occurence) of the observation falling in that region. Therefore prediction is made with the observations that happen most often and not everything as in regression tree.\n",
    "\n",
    "The splitting continues until a user defined stopping criteria is reached. But the fully grown tree is likely to overfit leading to poor accuracy on unseen data and this brings __PRUNING__\n",
    "\n",
    "__Pruning__ is one to the techiniques to tackle overfitting. Another way to overcome overfitting in Decision tree is to use the __ENSEMBLE METHOD__\n",
    "Decision trees are very useful when used with other machine learning algorithms like random forest and boosting.\n",
    "\n",
    "Growing a tree involves:\n",
    "1. Features to choose\n",
    "2. Conditions for splitting\n",
    "3. Knowing when to stop\n",
    "4. Pruning\n",
    "\n",
    "#### Common decision tree algorithm\n",
    "1. Giri index\n",
    "2. Chi-square\n",
    "3. Information gain\n",
    "4. Reduction in variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(max_depth=4)  # initializing and pruning to avoid overfitting\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Prediction accuracy of test data:', accuracy)\n",
    "\n",
    "y_pred_train = classifier.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_pred_train, y_train)\n",
    "print('Prediction accuracy of train data:', accuracy_train)\n",
    "\n",
    "# compare both accuracies to determine if there is overfitting, underfitting, or model is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Works amazingly well in NLP. It predicts the likelihood/probabilty of B given A i.e.\n",
    "\n",
    "    P(B|A)\n",
    "    \n",
    "    # to build a naive bayes model\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gnb = GaussianNB()  # initialize\n",
    "    gnb.fit(X_train, y_train)  # fit/model\n",
    "    y_gauss_pred = gnb.predict(X_test)  # predict with test data\n",
    "    y_gauss_train_pred = gnb.predict(X_train)  # predict with train data\n",
    "    \n",
    "    # accuracy\n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_gauss_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_gauss_train, y_gauss_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "It's a powerful algorithm suited for extreme cases e.g. trying to identify if a supper fluffy animal at the park is a dog or a cat\n",
    "\n",
    "It draws a hyperplane at the extreme data points aka Support vectors. SMV is a frontier of which best segregates the two classes(HYPER-PLANE/LINE).\n",
    "\n",
    "Using the optimal separation/hyper plane is important because unoptimized decision boundary could result in greater miscalculations on new data.\n",
    "\n",
    "Support Vectors are data points that the margins push against or points that are closes to the opposing class. So the algorithms implies that only support vectors are important whereas other training examples are ignorable.\n",
    "\n",
    "So we have \n",
    "1. D+ which is the shortest distance to the closest positive point, and \n",
    "2. D-ve which is the shortest distance to the closest negative point, and\n",
    "3. Margin of a separator hyper-plane which is \n",
    "         Margin = D+ + D-ve\n",
    "\n",
    "The line or boundary that separates the 2 classes is commonly referred to as a Hyper-plane because SVMs can be used for multiple applications and the data points are referred to as vectors.\n",
    "\n",
    "Types of SVM\n",
    "1. Linear Support Vector Machine (LSVM) - the classes are linearly separable\n",
    "When we have Non-linear SVM i.e. when it's impossible to use a straight line to separate the classes because it will go through data points for both classes, we can use a function to project our data into high dimensional space i.e. 1D to 2D or 2D to 3D and so on.\n",
    "\n",
    "The only thing with projecting to a higer dimension is that it is computationaly expensive. We can use a __kernel trick__ to reduce the computational cost. A function that takes as its inputs vectors in the original space and returns the dot product of the vectors in the feature space is called a __kernel function__, also referred to as a kernel trick.\n",
    "\n",
    "Using a kernel function, we can apply the dot product between 2 vectors so that every point is mapped into a high dimentional space via some transformation. i.e. we use it to transform a non linear space into a linear space.\n",
    "\n",
    "#### Popular Kernel Types\n",
    "which we can use to transform our data into higher dimensional feature space\n",
    "- Polynomial kernel\n",
    "- Radian basis function RBF kernel\n",
    "- Sigmoid Kernel \n",
    "- liner etc.\n",
    "\n",
    "Deciding on the kernel type  to use a non trivial task because no matter the kernel type you use, you need to __tune the kernel parameters__ to get good performance from a classifier. \n",
    "\n",
    "When solving a linear problem, using kernel='linear' when initializing the classifier could improve the accuracy score.\n",
    "\n",
    "### Popular Parameter Tuning techniques\n",
    "1. K-fold\n",
    "2. Cross validation\n",
    "\n",
    "\n",
    "#### Advantages of SVM\n",
    "- They are effective in high dimension spaces.\n",
    "- They are still effective where the number of dimensionns are greater than the number of samples.\n",
    "- They use a subset of training points in the decision function or support vectors so it's also memory efficient.\n",
    "- They are versatile so different kernels can be specified for the decision function.\n",
    "- Common kernels are provided but it's also possible to specify custom kernels.\n",
    "- We can add kernel functions together to achieve even more complex hyper planes.\n",
    "\n",
    "#### Disadvantages\n",
    "- If the number of features is greater than the number of samples, the method is likely to give poor performance.\n",
    "- SVMs do not directly provide probability estimates. These are calculated using expensive 5 fold cross validation (K-fold cross validation).\n",
    "\n",
    "#### Applications\n",
    "It has numerous applications and can be quite popular with neural networks e.g. medical imaging, air quality, image interpolation, time-series predictions, machine fault diagnosis, web page ranking algorithm etc.\n",
    "\n",
    "\n",
    "When modeling with SVM, the default kernal in sklearn is 'rbf'. You can specify a different kernel when initializing the Support Vector Classifier i.e. svc_model = SVC(kernel = 'newkernerlyouwanttouse')\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()  # initilize the support vector classifier\n",
    "    svc_model.fit(X_train, y_train)  # model\n",
    "    y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "    \n",
    "    y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "    \n",
    "    # check the accuracy\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM example\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC()  # initilize the support vector classifier\n",
    "svc_model.fit(X_train, y_train)  # model\n",
    "y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "\n",
    "y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "\n",
    "# check the accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Testing data accuracy score')\n",
    "print(metrics.accuracy_score(y_test, y_pred)\n",
    "print('=============================')\n",
    "print('Training data accuracy score')\n",
    "print(metrics.accuracy_score(y_train, y_train_pred))\n",
    "      \n",
    "      \n",
    "# to view the boundary - first create a mesh i.e. the axis tick/grid values\n",
    "X = X = iris.data[:, :2]    # matplotlib is for 2D maps so you need to specify [:, :2] where :2 will select 'Sepal Length' and 'Sepal width'\n",
    "y = iris.target\n",
    "      \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = (x_max / x_min)/100\n",
    "xx, yy = np.meshgrid(np.arrange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "                     \n",
    "plt.subplot(1, 1, 1)\n",
    "Z = svc_iris_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpa=0.8)  # to show the boundary/plane of separation cmap=plt.cm.Paired means to make the data points same color as the plane colors\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.title('SVC with linear kernel')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "- It is a technique used for improving prediction accuracy\n",
    "- It uses multpl learning lgorithms instead of simgle algorithm\n",
    "\n",
    "You use multiple homogenous algorithms i.e multiple weak learners are combined to form a strong learner i.e. more generalized rather than overfitted.\n",
    "\n",
    "2 Commonly used ensemble methods are:\n",
    "- Bagging/Boostrap aggregating \n",
    "- Boosting\n",
    "\n",
    "Bagging is a machine learning ensemble meta-algorithm designed to improve the stability, reduce variance, and accuracy.\n",
    "\n",
    "Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning. Algorithms that achieve hypothesis boosting simply become known as boosting.\n",
    "\n",
    "\n",
    "### Bagging\n",
    "Example of Bagging is __Random Forest Classifier__ (n nnumber of decision trees put together). Random Forest is a bagging technique based on decision tree algorithm. The input data to the algorithm is sub sampled (with __replacement__) and sent to every decision tree initialized earlier at a __random__ fashion.\n",
    "\n",
    "It grows multiple decision trees (as opposed to a single tree) using algorithms such as:\n",
    "- information gain\n",
    "- Gini index approach\n",
    "- other decision tree algorithms\n",
    "\n",
    "Each tree gives a classification and we say that the tree votes for that class. The classification with the most votes is chosen. In the case of regression, it takes the average of the outputs by different trees.\n",
    "\n",
    "__Note that along with the features, the target will also be passed as part of the data for the decision trees__\n",
    "\n",
    "## Random Forest Classifier\n",
    "While initializing this algorithm, the number of decision trees has to be specified.\n",
    "\n",
    "#### Advantages\n",
    "- Can be used for both classification and regression tasks. \n",
    "- Will handle the missing values and maintains accuracy when a large proportion of the data are missing data\n",
    "- Won't overfit the model \n",
    "- Handle large dataset with higher dimensionality\n",
    "\n",
    "#### Disadvantages\n",
    "- Good job at classification but not as good for regression problems (as it does not give continuous nature of predictions)\n",
    "- You have very little control on what the model does. It can seem like a black box to statisticians. You can try different parameters and random seeds.\n",
    "\n",
    "- Banking - identify Loyal cxs and fraudulent cxs\n",
    "- Medicine - identiy disease by analyzing the patients medical records\n",
    "- Identify stock behavior and expected loss/profit by buying a particular stock\n",
    "- Likelihood of a customer liking the recommened product\n",
    "- Image classification\n",
    "- Lip reading and Voice classification\n",
    "\n",
    "#### Random Forest Pseudocode\n",
    "1. Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replcement.\n",
    "\n",
    "2. If there are M input variable or features, a number m<M is specified such that at each node, m variables are selected at random our of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "\n",
    "3. Each tree is grown to the largest ectent possible and __there is no pruning__\n",
    "\n",
    "4. Predict new data by aggregating the predictions of the n tree trees (i.e. majority votes for classification, average for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=150, random_state=21)  # initialize\n",
    "# the random_state above should be removed if you already used seed in your split else you would need to specify it in every algorithm you run\n",
    "rfc.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_predict, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# you will see an increased accuracy compared to decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Considers homogenous weak learners and learns from them __sequentially__ (each trying to correct its predecessor) unlike bagging where the learning is in parallel. Here the learning is also adaptive--the base model depends on the previous model's output.\n",
    "\n",
    "#### Advantage\n",
    "It reduces bias and increases prediction accuracy\n",
    "\n",
    "### Boosting Algorithms\n",
    "1. Adaboost classifier i.e Adaptive boost\n",
    "2. Gradient boost (GBM)\n",
    "3. __XGBoost__ (Extreme Gradient Boost)\n",
    "\n",
    "### Adaboost\n",
    "Step 1:\n",
    "- Assign equal wieght to each data point\n",
    "- Apply a decision stump to classify them as +(plus) and -(minus)\n",
    "\n",
    "__A stump__ is a decision tree with depth = 1 i.e. there is only one level after the root node. The entropy of the stumps are compared and the stump with the lowest gini value will be used as the base learner model. \n",
    "\n",
    "Step 2:\n",
    "- size of incorrectly predicted points is made bigger compared to the rest of the data points\n",
    "- The second decision stump will try to predict them correctly\n",
    "The iteration and weighting continues until the class is determined i.e for every misclassified point, the weight is increased to get the point included\n",
    "\n",
    "### Gradient Boosting\n",
    "The objective is to reduce the residuals/losses by finding the optimal fit.\n",
    "It involves three elements:\n",
    "1. A loss function to be optimized\n",
    "2. A weak learner to make predictions\n",
    "3. An additive model to add weak learners to minimize the loss function\n",
    "\n",
    "GBM minimizes the loss function (MSE) of a model by adding weak learners using a gradient descent procedure.\n",
    "\n",
    "Steps for GBM\n",
    "1. Fit a simple regression or classification model\n",
    "2. Calculate error residuals (actual value - predicted value)\n",
    "3. Fit a new model on error residuals as target variable with same imput variable\n",
    "4. Add the predicted residuals to the previous predictions\n",
    "5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until model lis overfit or the sum of residuals become constant.\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "- It is extensively used in ML competitions as it is 10 times faster than other gradient boosting techniques. \n",
    "- More powerful than SVM. \n",
    "- Used for classification, regression, and ranking. \n",
    "- IInterfaces for R and Python and can be executed on  YARN - Yet Another Resource Negotiator - a big data platform\n",
    "- When training very large models, it can work with clusters of machines\n",
    "- Has DMatrix structure so the data has to be converted to this data structure before applying XGBoost\n",
    "- Automatic handling of missing data values\n",
    "- Supports the parallization of tree construction\n",
    "- Boost an already fitted model on new data\n",
    "\n",
    "__XGBoost models__\n",
    "1. __Gradient Boosting__ - learning rate\n",
    "2. __Stochastic Gradient Boosting__\n",
    "3. __Regularized Gradient Boosting__ with both L1(lasso) and L2 regularization\n",
    "\n",
    "__General Parameters of XGBoost__\n",
    "1. nthread - number of parallel threads. If no value is entered, the algorithm automatically detects the number of cores and runs on all the cores (i.e. your CPU)\n",
    "2. __booster__\n",
    "  - __gbtree__: tree-based model - for classification\n",
    "  - __gblinear__: linear function - for regression\n",
    "3. Silent[default=0]\n",
    "  - if set to 1, no running messages will be printed. Hence, _keep it '0' as the messages might help in understanding the model_.\n",
    "  \n",
    "__Booster Parameters__\n",
    "Booster parametes guide individual booster at each step i.e the tree/linear (gbtree/gblinear)\n",
    "1. __eta__: is the learning rate i.e step size shrinkage is used in update to prevent overfitting. Range is [0,1] default 0.3 __Note*: Use values 0.1, 0.01, 0.2, 0.02. Never pick learning rate/eta of 1__\n",
    "\n",
    "2. __gamma__: minimum loss reduction required to make a split\n",
    "    - Range[0, infinity], default is 0 -- *Use default\n",
    "3. __max_depth__: maximum depth of a tree i.e levels \n",
    "    - Range[1, infinity] default 6  -- even though the range is 1-infinity, __*use 3 to 10 especially if going to a hackerton__\n",
    "4. __min_child_weight__: minimum sum of instance weight needed in a child\n",
    "    - Range[0, infinity] default 1 -- Use default value of 1\n",
    "\n",
    "#### Note: the tree splits from the root node when the entropy INCREASES/information gain REDUCES/loss reduction BEGINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive boosting -adaboost example\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)\n",
    "\n",
    "\n",
    "#adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_model = AdaBoostClassifier(n_estimators = 80)  # the base estimator is a weak learner e.g. decision tree classifier \n",
    "ada_model.fit(X_train, y_train)  # you need to pass y data because it's a supervised learning\n",
    "y_pred= ada_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy score\n",
    "print('Accuracy of test data:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# You will notice that the accuracy is really high as though it's SVM. \n",
    "\n",
    "y_train_pred = ada_model.predict(X_train)\n",
    "print('Accuracy of train data:', accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "# There is overfitting but not as bad as decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost \n",
    "# Note: needs to be installed if you haven't done so\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# assign X and y\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_size=21)\n",
    "\n",
    "# building the xgboost model\n",
    "# for efficient use of the xgboost model, we are going to convert our dataset to the DMatrix format\n",
    "# DMatrix is the data structure unique for xgboost algorithm\n",
    "\n",
    "D_train = xgb.DMatrix(X_train, label=y_train)  # the label needs to be used to convert train data to DMatrix\n",
    "D_test = xgb.DMatrix(X_test) # label is not used to covnert the test data\n",
    "\n",
    "# initialize a set of parameters using a dictionary because there are many parameters\n",
    "param = {'eta':0.02, 'max_depth':4, 'objective':'multi:softmax', 'num_class':3 }  # num_class is 3 because we have 3 classes in the iris dataset. Recall that anytime we specify objective as multi:softmax (classes > 2), we also need to specify the num_class\n",
    "\n",
    "xgb_model = xgb.train(param, D_train, 20)  # 20 is the no of iterations you want it to do before providing a result\n",
    "\n",
    "# predict\n",
    "y_xgb_pred = xgb_model.predict(D_test)\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('The accuracy of xgb model on iris dataset is')\n",
    "print(accuracy_score(y_test, y_xgb_pred))\n",
    "\n",
    "# enhance model\n",
    "# the accuracy of 91.11% can be improved by adjusting the parmeters and building the model again\n",
    "\n",
    "param2 = {'eta':0.3, 'max_depth':3, 'objective':'multi:softmax', 'num_class':3 }  # num_class is 3 because we have 3 classes in the iris dataset. Recall that anytime we specify objective as multi:softmax (classes > 2), we also need to specify the num_class\n",
    "xgb_model2 = xgb.train(param, D_train, 20) \n",
    "y_xgb_pred2 = xgb_model2.predict(D_test)\n",
    "print('The accuracy of xgb model on iris dataset is')\n",
    "print(accuraccy_score(y_test, y_xgb_pred2))\n",
    "\n",
    "\n",
    "# you can see that the accuracy increased to 93.33%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "1. Leave one out cv - very time consumming and not used anymore\n",
    "2. K-fold cv - There may be an inbalance in selection where data points of same class may be dominant in the test or train data\n",
    "3. __stratified cross validation__ - ensures that the no of instances of each class for train and test is properly taken to fix the problem experienced with k-fold. Also recall that feature imbalance should be fixed in feature engineering so that is different from what we are talking about here.\n",
    "4. Time Series cv - for data set with time/time series related dataset. You cannot do a train/test split with time series data (https://www.youtube.com/watch?v=7062skdX05Y)\n",
    "\n",
    "__CV is an alternative to a train/test split ratio__\n",
    "\n",
    "In a normal train_test_split, you split the data into percentages. With cross validation all the data is used for train as well as test by splitting the data into a specified number e.g. 4. 3 parts will be used for training and 1 part for testing and in the next iteration, one of the parts used in training will be used for testing and the remaining for training. The iteration will continue untill all of the 4 parts have been used for testing...i.e. k number and the number of iterations is equal.\n",
    "\n",
    "__k-fold cross validation ------ leave one out for testing__\n",
    "\n",
    "\n",
    "CV is very helpful when you don't have a huge dataset. e.g. you have 300 observations for 100 possible outcomes which means you have only 3 records for each outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold cross validation with KNN model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import the model you want to use\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "cross_val_score(knn_model, X, y, cv=5, scoring='accuracy').mean()  # pass the model you want to use to train, pass the whole X and y without spliting, cv of 5 means it will split the data into 10 so there will be 10 iterations, the scoring will be required and finally, you specify the mean\n",
    "# scoring is used for accuracy. You could use rmse when using a the regression model\n",
    "# you will get an accuracy value from above stop\n",
    "# you can change the cv values to see if there is an improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's demonstrate the cross validation with logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "cross_val_score(logreg, X, y, cv=8, scoring='accuracy').mean()\n",
    "\n",
    "# You will get a warning that you've exceeded the limit for logistic regression. Recall that it is better for 2 class classification\n",
    "# You can try using Random Forest and other model types with k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Cross Validation\n",
    "\n",
    "- This has nothing to do with splitting the data rather, it's for __best parameter value selection__\n",
    "- It can be used to tune the hyperparameters of a model\n",
    "- It loops through predefined hyperparameters and will fit the estimator model on the training data set\n",
    "- At the end, you can select the best hyperparameter from the predefined list based on the performance\n",
    "- It's in the model selection package of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# initialize the model object you want to use\n",
    "knn_gscv = KNeighborsClassifier()\n",
    "\n",
    "# specify the parameter values to try\n",
    "params = {'n_neighbors':[4, 5, 6, 10, 15],\n",
    "         'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "         }\n",
    "\n",
    "# there are 5*2*4 = 40 values in our params so that's 40 iterations. GridSearch saves us the stress of manually changing a value and reruning the model 40 times\n",
    "\n",
    "grid_knn = GridSearchCV(estimator=knn_gscv, param_grid=params, cv=5, scoring='accuracy')  # this is like your initialized model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# fit the model\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print('The best parameter values of KNN model for this dataset are:')\n",
    "print(grid_knn.best_params_)\n",
    "\n",
    "# predict\n",
    "y_pred = grid_knn.predict(X_test)\n",
    "\n",
    "# accuracy score\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# another way to check the accuracy score instead of using the accuracy_score and y_pred\n",
    "# grid_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
