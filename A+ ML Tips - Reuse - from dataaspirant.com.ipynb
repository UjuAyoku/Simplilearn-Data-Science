{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "### Data Imputation:\n",
    "\n",
    "Imputation is a process of replacing missing values with substituted values. In our dataset, some columns have missing values. We can replace missing values with mean, median, mode or any particular value.\n",
    "Sklearn provides Imputer() method to perform imputation in 1 line of code. We just need to define missing_values, axis, and strategy. We are using “median” value of the column to substitute with the missing value.\n",
    "\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values=\"NaN\", strategy='median', axis=0)\n",
    "    X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection\n",
    "\n",
    "correlation with dependent variable\n",
    "\n",
    "    cor = train_df.corr()\n",
    "    cor_target = abs(cor['y'])\n",
    "\n",
    "selecting highly correlated features\n",
    "\n",
    "    relevant_features = cor_target[cor_target>0.5]\n",
    "    relevant_features\n",
    "    \n",
    "Recall that for linear regression, __multicolinearity__ is a big issue so you need to also check the correlations among the features and if 2 features are correlated, you drop one. If you have 1 feature correlating with more than 1 features, that will be the best feature to drop.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score\n",
    "\n",
    "__accuracy_score()__: This function is used to print accuracy of KNN algorithm. By accuracy, we mean the ratio of the correctly predicted data points to all the predicted data points. Accuracy as a metric helps to understand the effectiveness of our algorithm. It takes 4 parameters.\n",
    "\n",
    "- y_true\n",
    "- y_pred\n",
    "- normalize\n",
    "- sample_weight\n",
    "\n",
    "Out of these 4, normalize & sample_weight are optional parameters. The parameter y_true  accepts an array of correct labels and y_pred takes an array of predicted labels that are returned by the classifier. It returns accuracy as a float value.\n",
    "\n",
    "\n",
    "    # to check the accuracy score\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "In addition to the accuracy_score, the confusion matrix can also be used to check the accuracy of the model prediction.\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # print the confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # print the classification report\n",
    "    print(classification_report((y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using KNN modeling example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for K in range(15):  # generate a range of numbers for k i.e. from 1 - 15\n",
    "    K_value = K+1\n",
    "    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n",
    "    neigh.fit(X_train, y_train) \n",
    "    y_pred = neigh.predict(X_test)\n",
    "    print \"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\",K_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K value Vs Accuracy Change Graph\n",
    "\n",
    "Plot the Accuracy on y axis and K-value on x axis to see the optimum accuracy\n",
    "\n",
    "Fig: K-value Vs Accuracy\n",
    "\n",
    "It shows that we are getting 95.71% accuracy on K = 3, 5. Choosing a large value of K will lead to greater amount of execution time & underfitting. Selecting the small value of K will lead to overfitting. There is no such guaranteed way to find the best value of K. So, to run it quickly we are considering K =3 for this tutorial.\n",
    "\n",
    "__Note__: Implementing KNN on cross-validation data. It helps to validate which K value may give the better accuracy.\n",
    "\n",
    "A very low K value may be overfitted while a high k value defeats the purpose as boundaries will be crossed, thereby reducing the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib - Fit curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# example data\n",
    "mu = 100 # mean of distribution\n",
    "sigma = 15 # standard deviation of distribution\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    " \n",
    "num_bins = 50\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, num_bins, density=1, facecolor='green', alpha=0.5)\n",
    "# add a 'best fit' line\n",
    "y = norm.pdf(bins, mu, sigma)\n",
    "plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')\n",
    " \n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "https://dataaspirant.wordpress.com/2014/10/02/linear-regression/\n",
    "\n",
    "Linear Regression means predicting scores of one variable from the scores of second variable. The variable we are predicting is called the __criterion/depenndent/output variable__ and is referred to as Y. The variable we are basing our predictions is called the __predictor/independent/input variable__ and is referred to as X. \n",
    "\n",
    "When there is only one predictor variable, the prediction method is called __simple regression__.\n",
    "\n",
    "The aim of linear regression is to finding the best-fitting straight line through the points. The best-fitting line is called a __regression line.__\n",
    "\n",
    "       hθ(x) = θ0 + θ1X\n",
    "The above equation is __hypothesis equation__\n",
    "\n",
    "where:\n",
    "\n",
    "hθ(x) is nothing but the value Y(*which we are going to predicate* )  for particular x ( means Y is a linear function of x)\n",
    "\n",
    "θ0 is a constant equal to the y intercept of the regression line\n",
    "\n",
    "θ1 is  the __regression coefficient__ i.e.  the average change in the dependent variable (Y) for a 1-unit change in the independent variable (X). It is the slope of the regression line\n",
    "\n",
    "X is value of the independent variable\n",
    "\n",
    "### Properties of the Linear Regression Line\n",
    "Linear Regression line has the following properties:\n",
    "\n",
    "1. The line minimizes the sum of squared differences between observed values (the y values) and predicted values (the hθ(x) values computed from the regression equation).\n",
    "2. The regression line passes through the mean of the X values (x) and through the mean of the Y values ( hθ(x) ).\n",
    "3. The regression constant (θ0) is equal to the y intercept of the regression line.\n",
    "4. The regression coefficient (θ1) is the average change in the dependent variable (Y) for a 1-unit change in the independent variable (X). It is the slope of the regression line.\n",
    "\n",
    "#### The least squares regression line is the only straight line that has all of these properties.\n",
    "\n",
    "\n",
    "### Goal of  Hypothesis Function\n",
    "Goal of Hypothesis is to choose θ0 and θ1 , so that hθ(x) is close to Y for our training data,while choosing θ0 and θ1 we have to consider the cost function( J(θ) ) where we are getting low value for cost function( J(θ) ).\n",
    "\n",
    "The below function is called as cost function, cost function ( J(θ) ) is nothing but just a __Squared error function.__\n",
    "\n",
    "\n",
    "### Strategy\n",
    "- Start with some θ0, θ1\n",
    "- Keepchanging θ0, θ1 to reduce J(θ0,θ1) until we hopefully end up at a minimum\n",
    "\n",
    "https://dataaspirant.com/linear-regression-implementation-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show the resutls of linear fit model\n",
    "def show_linear_line(X_parameters,Y_parameters):\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_parameters, Y_parameters)\n",
    "    plt.scatter(X_parameters,Y_parameters,color='blue')\n",
    "    plt.plot(X_parameters,regr.predict(X_parameters),color='red',linewidth=4)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()   \n",
    "    \n",
    "\n",
    "# to call the show linear line function\n",
    "show_linear_line(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Vs Clustering\n",
    "\n",
    "https://dataaspirant.com/classification-clustering-alogrithms/\n",
    "\n",
    "https://sites.google.com/site/dataclusteringalgorithms/home\n",
    "\n",
    "## Classification Concept\n",
    "In classification, the idea is to predict the target class by analysis the training dataset. This could be done by finding proper boundaries for each target class. In a general way of saying, Use the training dataset to get better boundary conditions which could be used to determine each target class. Once the boundary conditions determined, the next task is to predict the target class as we have said earlier. The whole process is known as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithms\n",
    "- Linear classifiers\n",
    "    - Logistic regression\n",
    "    - Naive Bayes classifier\n",
    "    - Fisher’s linear discriminant\n",
    "- Support vector machines\n",
    "    - Least squares support vector machines\n",
    "- Quadratic classifiers\n",
    "- Kernel estimation\n",
    "    - k-nearest neighbor\n",
    "- Decision trees\n",
    "    - Random forests\n",
    "- Neural networks\n",
    "- Learning vector quantization\n",
    "\n",
    "### Application of Classification Algorithms\n",
    "- Email spam classification\n",
    "- Bank customers loan pay bank willingness prediction.\n",
    "- Cancer tumour cells identification.\n",
    "- Sentiment analysis.\n",
    "- Drugs classification\n",
    "- Facial key points detection\n",
    "- Pedestrians detection in an automotive car driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Concept\n",
    "\n",
    "In clustering the idea is not to predict the target class as like classification , it’s more ever trying to group the similar kind of things by considering the most satisfied condition __all the items in the same group should be similar and no two different group items should not be similar__.  To group the similar kind of items in clustering, different similarity measures could be used i.e similarity measure is used to cluster.\n",
    "\n",
    "### Group items Examples:\n",
    "While grouping similar language type documents (Same language documents are one group.)\n",
    "While categorising the news articles (Same news category(Sport) articles are one group )\n",
    "\n",
    "\n",
    "## Clustering Algorithms\n",
    "Clustering algorithms can be classified into two main categories \n",
    "- Linear clustering algorithms and \n",
    "- Non-linear clustering algorithms.\n",
    "\n",
    "### Linear clustering algorithm\n",
    "- k-means clustering algorithm\n",
    "- Fuzzy c-means clustering algorithm\n",
    "- Hierarchical clustering algorithm\n",
    "- Gaussian(EM) clustering algorithm\n",
    "- Quality threshold clustering algorithm\n",
    "\n",
    "### Non-linear clustering algorithm\n",
    "- MST based clustering algorithm\n",
    "- kernel k-means clustering algorithm\n",
    "- Density-based clustering algorithm\n",
    "\n",
    "### Application of Clustering Algorithms\n",
    "- Recommender systems\n",
    "- Anomaly detection\n",
    "- Human genetic clustering\n",
    "- Genom Sequence analysis\n",
    "- Analysis of antimicrobial activity\n",
    "- Grouping of shopping items\n",
    "- Search result grouping\n",
    "- Slippy map optimization\n",
    "- Crime analysis\n",
    "- Climatology\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "__Classification__: Predicting target class for test dataset from the trained modeled from the training dataset.\n",
    "\n",
    "__Clustering__: Using different similarity measure to place the all the similar items in a group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Concept\n",
    "\n",
    "https://dataaspirant.com/how-logistic-regression-model-works/\n",
    "\n",
    "“Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities __(likelihood occurrence)__ using a __logistic function__” (Wikipedia)\n",
    "\n",
    "It uses a black box function to understand the relation between the categorical __dependent variable__ and the __independent variables__. This black box function is popularly known as the __Softmax funciton__.\n",
    "\n",
    "#### Two functions which determine the logistic regression model. Just for a glance:\n",
    "\n",
    "- __Softmax__: Used for the multi-classification task.\n",
    "     - The probabilities sum will be 1\n",
    "     - Used in the different layers of neural networks.\n",
    "     - The high value will have the higher probability than other values.\n",
    "- __Sigmoid__: Used for the binary classification task.\n",
    "     - The probabilities sum need not be 1.\n",
    "     - Used as activation function while building neural networks.\n",
    "     - The high value will have the high probability but not the higher probability.\n",
    "\n",
    "https://dataaspirant.com/difference-between-softmax-function-and-sigmoid-function/\n",
    "\n",
    "### Examples of likelihood occurrence of an event\n",
    "- How likely a customer will buy iPod having iPhone in his/her pocket.\n",
    "- How likely Argentina team will win when Lionel Andrés Messi in rest.\n",
    "- What is the probability to get into best university by scoring decent marks in mathematics, physics?\n",
    "- What is the probability to get a kiss from your girlfriend when you gifted her favorite dress on behalf of your birthday?\n",
    "\n",
    "If the logistic regression model used for addressing the binary classification kind of problems it’s known as the __binary logistic regression classifier__. Whereas the logistic regression model used for multiclassification kind of problems, it’s called the __multinomial logistic regression classifier__ i.e. using the logistic regression techniques to predict target with __more than 2 target classes__.\n",
    "\n",
    "The underline technique will be same like the logistic regression for binary classification until calculating the probabilities for each target. Once the probabilities were calculated. We need to transfer them into __one hot encoding__ and uses the cross entropy methods in the training process for calculating the properly optimized weights.\n",
    "\n",
    "Multinomial logistic regression works well on big data irrespective of different areas. Surprisingly it is also used in human resource development and more in depth details about how the big data is used in human resource development can found in this article.\n",
    "\n",
    "https://dataaspirant.com/multinomial-logistic-regression-model-works-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://dataaspirant.com/implement-logistic-regression-model-python-binary-classification/\n",
    "    \n",
    "# About: Implementing Logistic Regression Classifier to predict to whom the voter will vote.\n",
    " \n",
    "# Required Python Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    " \n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "py.sign_in('dataaspirant', 'RhJdlA1OsXsTjcRA0Kka')\n",
    " \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    " \n",
    "    \n",
    "# file \n",
    "DATA_SET_PATH = \"../Inputs/anes_dataset.csv\"\n",
    " \n",
    " \n",
    "def dataset_headers(dataset):\n",
    "        \"\"\"\n",
    "    To get the dataset header names\n",
    "    :param dataset: loaded dataset into pandas DataFrame\n",
    "    :return: list of header names\n",
    "    \"\"\"\n",
    "    return list(dataset.columns.values)\n",
    " \n",
    " \n",
    "def unique_observations(dataset, header, method=1):\n",
    "    \"\"\"\n",
    "    To get unique observations in the loaded pandas DataFrame column\n",
    "    :param dataset:\n",
    "    :param header:\n",
    "    :param method: Method to perform the unique (default method=1 for pandas and method=0 for numpy )\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method == 0:\n",
    "            # With Numpy\n",
    "            observations = np.unique(dataset[[header]])\n",
    "        elif method == 1:\n",
    "            # With Pandas\n",
    "            observations = pd.unique(dataset[header].values.ravel())\n",
    "        else:\n",
    "            observations = None\n",
    "            print \"Wrong method type, Use 1 for pandas and 0 for numpy\"\n",
    "    except Exception as e:\n",
    "        observations = None\n",
    "        print \"Error: {error_msg} /n Please check the inputs once..!\".format(error_msg=e.message)\n",
    "    return observations\n",
    " \n",
    " \n",
    "def feature_target_frequency_relation(dataset, f_t_headers):\n",
    " \n",
    "    \"\"\"\n",
    "    To get the frequency relation between targets and the unique feature observations\n",
    "    :param dataset:\n",
    "    :param f_t_headers: feature and target header\n",
    "    :return: feature unique observations dictionary of frequency count dictionary\n",
    "    \"\"\"\n",
    " \n",
    "    feature_unique_observations = unique_observations(dataset, f_t_headers[0])\n",
    "    unique_targets = unique_observations(dataset, f_t_headers[1])\n",
    " \n",
    "    frequencies = {}\n",
    "    for feature in feature_unique_observations:\n",
    "        frequencies[feature] = {unique_targets[0]: len(\n",
    "            dataset[(dataset[f_t_headers[0]] == feature) & (dataset[f_t_headers[1]] == unique_targets[0])]),\n",
    "            unique_targets[1]: len(\n",
    "                dataset[(dataset[f_t_headers[0]] == feature) & (dataset[f_t_headers[1]] == unique_targets[1])])}\n",
    "    return frequencies\n",
    " \n",
    " \n",
    "def feature_target_histogram(feature_target_frequencies, feature_header):\n",
    "    \"\"\"\n",
    " \n",
    "    :param feature_target_frequencies:\n",
    "    :param feature_header:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    keys = feature_target_frequencies.keys()\n",
    "    y0 = [feature_target_frequencies[key][0] for key in keys]\n",
    "    y1 = [feature_target_frequencies[key][1] for key in keys]\n",
    " \n",
    "    trace1 = go.Bar(\n",
    "        x=keys,\n",
    "        y=y0,\n",
    "        name='Clinton'\n",
    "    )\n",
    "    trace2 = go.Bar(\n",
    "        x=keys,\n",
    "        y=y1,\n",
    "        name='Dole'\n",
    "    )\n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(\n",
    "        barmode='group',\n",
    "        title='Feature :: ' + feature_header + ' Clinton Vs Dole votes Frequency',\n",
    "        xaxis=dict(title=\"Feature :: \" + feature_header + \" classes\"),\n",
    "        yaxis=dict(title=\"Votes Frequency\")\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    # plot_url = py.plot(fig, filename=feature_header + ' - Target - Histogram')\n",
    "    py.image.save_as(fig, filename=feature_header + '_Target_Histogram.png')\n",
    " \n",
    " \n",
    "def train_logistic_regression(train_x, train_y):\n",
    "    \"\"\"\n",
    "    Training logistic regression model with train dataset features(train_x) and target(train_y)\n",
    "    :param train_x:\n",
    "    :param train_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    " \n",
    "    logistic_regression_model = LogisticRegression()\n",
    "    logistic_regression_model.fit(train_x, train_y)\n",
    "    return logistic_regression_model\n",
    " \n",
    " \n",
    "def model_accuracy(trained_model, features, targets):\n",
    "    \"\"\"\n",
    "    Get the accuracy score of the model\n",
    "    :param trained_model:\n",
    "    :param features:\n",
    "    :param targets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    accuracy_score = trained_model.score(features, targets)\n",
    "    return accuracy_score\n",
    " \n",
    " \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier main\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the data set for training and testing the logistic regression classifier\n",
    "    dataset = pd.read_csv(DATA_SET_PATH)\n",
    "    print \"Number of Observations :: \", len(dataset)\n",
    " \n",
    "    # Get the first observation\n",
    "    print dataset.head()\n",
    " \n",
    "    headers = dataset_headers(dataset)\n",
    "    print \"Data set headers :: {headers}\".format(headers=headers)\n",
    " \n",
    "    training_features = ['TVnews', 'PID', 'age', 'educ', 'income']\n",
    "    target = 'vote'\n",
    " \n",
    "    # Train , Test data split\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[training_features], dataset[target], train_size=0.7)\n",
    "    print \"train_x size :: \", train_x.shape\n",
    "    print \"train_y size :: \", train_y.shape\n",
    " \n",
    "    print \"test_x size :: \", test_x.shape\n",
    "    print \"test_y size :: \", test_y.shape\n",
    " \n",
    "    print \"edu_target_frequencies :: \", feature_target_frequency_relation(dataset, [training_features[3], target])\n",
    " \n",
    "    for feature in training_features:\n",
    "        feature_target_frequencies = feature_target_frequency_relation(dataset, [feature, target])\n",
    "        feature_target_histogram(feature_target_frequencies, feature)\n",
    " \n",
    "    # Training Logistic regression model\n",
    "    trained_logistic_regression_model = train_logistic_regression(train_x, train_y)\n",
    "    \n",
    "    train_accuracy = model_accuracy(trained_logistic_regression_model, train_x, train_y)\n",
    " \n",
    "    # Testing the logistic regression model\n",
    "    test_accuracy = model_accuracy(trained_logistic_regression_model, test_x, test_y)\n",
    " \n",
    "    print \"Train Accuracy :: \", train_accuracy\n",
    "    print \"Test Accuracy :: \", test_accuracy\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22ff385bec8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required Packages\n",
    " \n",
    "from datetime import datetime\n",
    " \n",
    "x = [ \n",
    "datetime(year=2000,month=1,day=1),\n",
    "datetime(year=2001,month=1,day=1),\n",
    "datetime(year=2002,month=1,day=1),\n",
    "datetime(year=2003,month=1,day=1),\n",
    "datetime(year=2004,month=1,day=1),\n",
    "datetime(year=2005,month=1,day=1),\n",
    "datetime(year=2006,month=1,day=1),\n",
    "datetime(year=2007,month=1,day=1),\n",
    "datetime(year=2008,month=1,day=1),\n",
    "datetime(year=2009,month=1,day=1),\n",
    "datetime(year=2010,month=1,day=1),\n",
    "datetime(year=2011,month=1,day=1),\n",
    "datetime(year=2012,month=1,day=1)\n",
    "]\n",
    " \n",
    "y = [1014004000, 1029991000, 1045845000, 1049700000, 1065071000, 1080264000, 1095352000, 1129866000, 1147996000, 1166079000, 1173108000, 1189173000, 1205074000]\n",
    "\n",
    "plt.scatter(x, y, c='yellow', linewidths=0.5, edgecolors='black')\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the target variable for Bias\n",
    "\n",
    "If the target/dependent variable is __categorical__ i.e not numeric, you need to check that the values for the categorical data is not biased i.e. you don't have one label significantly more in number than the other(s).\n",
    "\n",
    "Method 1:\n",
    "    \n",
    "    print('Total number of labels: {}'.format(df.shape[0]))\n",
    "    print('Number of male: {}'.format(df[df.columnname == 'male'].shape[0]))\n",
    "    print('Number of female: {}'.format(df[df[columnname] == 'female'].shape[0]))\n",
    "\n",
    "where columnname refers to the dependent variable/column name. compare the numbers\n",
    "\n",
    "Method 2:   \n",
    "\n",
    "    df['columname'].value_counts().plot.bar()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Features\n",
    "\n",
    "Standardizing your features is part of EDA. It's used to scale the data between -1 and 1 so once you go through your data and see that some features are say in the thousands while others are in the hundreds or even less than 1, you'll need to normalize/standardize the data after splitting X and y.\n",
    "\n",
    "First Split X and y\n",
    "\n",
    "    X = df.drop('target_column', axis=1)\n",
    "    y = df['target_column']\n",
    "\n",
    "### label encode target column (y) if it's not numeric \n",
    "i.e. to convert it from label to numeric as ML only use numeric\n",
    "\n",
    "     from sklearn.preprocessing import LabelEncoder \n",
    "     label_encoder = LabelEncoder()  # initialize\n",
    "     y = label_encoder.fit_transform(y)\n",
    "     # y will become an ndarray\n",
    "     type(y)\n",
    "     \n",
    "### Standardize the features\n",
    "scale the data to be between -1 and 1\n",
    "\n",
    "      from sklearn.preprocessing import StandardScaler\n",
    "      scaler = StandardScaler()\n",
    "      scaler.fit(X)\n",
    "      X = scaler.transform(X)\n",
    "      \n",
    "Split your data into train and test and carry on with initializing the algorithm, fitting the data (model), make predictions using the test datav(X_test) and then another with the train data (X_train), and comparing accuracy of test (y_test, y_test_pred) and train data (y_train, y_train_pred) to check for overfitting. See example code using Support Vector Machines below:\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()  # initilize the support vector classifier\n",
    "    svc_model.fit(X_train, y_train)  # model\n",
    "    y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "    \n",
    "    y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "    \n",
    "    # check the accuracy\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Encoding is required because Machine Learning only deals with numbers so categorical data/labels cannot be used in our models without converting it to numbers first.\n",
    "\n",
    "## One Hot Encoding\n",
    "\n",
    "One hot encoding is used for encoding features and will create new columns for the classes within the column and assign a 1 for a value under its column and 0 to other columns.\n",
    "\n",
    "    pd.get_dummies(dataframe['column_name'])\n",
    "    \n",
    "To perform one hot encoding, create a list with the list of all the columns to encode and pass it where you have 'column_name' in above formula.\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "Label encoding is used for the target variable because it does not create new additional columns like one-hot-encoding. We cannot have additional columns for our target/dependent variable. The label usually starts from 0.\n",
    "\n",
    "     from sklearn.preprocessing import LabelEncoder \n",
    "     label_encoder = LabelEncoder()  # initialize\n",
    "     y = label_encoder.fit_transform(dataframe['column_name'])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features need to be converted into categorical data inorder to use decision tree \n",
    "\n",
    "#### Decision Tree Algorithm Advantages and Disadvantages\n",
    "__Pros:__\n",
    "- simple to understand, interpret, visualize\n",
    "- It performed variable screening or feature selection\n",
    "- It can handle both numerica and categorical data (CART)\n",
    "- It can handle multi output problems\n",
    "- requires little effor for data preparation\n",
    "- nonlinear relationship between parameters do not affect the performance\n",
    "\n",
    "__Cons:__\n",
    "- can create over complex trees that do not generalize the data well - overfitting.\n",
    "- can become unstable because small variations in data can result in a completely different tree being generated. This is called __variance__ and needs to be lowered by methods of __bagging__ and __boosting__.\n",
    "- greedy algorithms cannot be guaranteed to return the globally optimal decision tree. This can be mitigated by training multiple trees where features and samples are randomly selected with replacement.\n",
    "- Decision tree learners also create bias trees if some classes dominate it. It is therefore recommended to balance the data set prior to fitting with decision tree.\n",
    "\n",
    "__What is a greedy algorithm__\n",
    "As the name suggests, a greedy algorithm always makes the choice that seems to be the best at the moment. This means that it makes a locally-optimal choice in the hope that this choice will lead to a globally-optimal solution.\n",
    "\n",
    "\n",
    "#### Applications\n",
    "1. Customer will pay his insurance premium\n",
    "2. If male/female on the titanic, what are the chances of survival\n",
    "3. If a person is male/female based on height and wieight\n",
    "4. Price of a house based on the number of rooms as well as the floor size\n",
    "\n",
    "Decision tree is drawn like an upside down tree\n",
    "\n",
    "\n",
    "#### Differences and Similarities between Classification and Regression Trees (CART)\n",
    "\n",
    "Regression trees are used when the dependent variable is continuous.\n",
    "Classification trees are used when the dependent variable is categorical.\n",
    "\n",
    "In the case of regression trees, the value obtained by terminal nodes is the mean or average response of the observation falling in that region thus if an unseen data observation falls in that region will make it's prediction with a mean value.\n",
    "\n",
    "In the cases of classification trees, the value or class obtained is the mode (occurence) of the observation falling in that region. Therefore prediction is made with the observations that happen most often and not everything as in regression tree.\n",
    "\n",
    "The splitting continues until a user defined stopping criteria is reached. But the fully grown tree is likely to overfit leading to poor accuracy on unseen data and this brings __PRUNING__\n",
    "\n",
    "__Pruning__ is one to the techiniques to tackle overfitting. Another way to overcome overfitting in Decision tree is to use the __ENSEMBLE METHOD__\n",
    "Decision trees are very useful when used with other machine learning algorithms like random forest and boosting.\n",
    "\n",
    "Growing a tree involves:\n",
    "1. Features to choose\n",
    "2. Conditions for splitting\n",
    "3. Knowing when to stop\n",
    "4. Pruning\n",
    "\n",
    "#### Common decision tree algorithm\n",
    "1. Giri index\n",
    "2. Chi-square\n",
    "3. Information gain\n",
    "4. Reduction in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Works amazingly well in NLP. It predicts the likelihood/probabilty of B given A i.e.\n",
    "\n",
    "    P(B|A)\n",
    "    \n",
    "    # to build a naive bayes model\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gnb = GaussianNB()  # initialize\n",
    "    gnb.fit(X_train, y_train)  # fit/model\n",
    "    y_gauss_pred = gnb.predict(X_test)  # predict with test data\n",
    "    y_gauss_train_pred = gnb.predict(X_train)  # predict with train data\n",
    "    \n",
    "    # accuracy\n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_gauss_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_gauss_train, y_gauss_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "It's a powerful algorithm suited for extreme cases e.g. trying to identify if a supper fluffy animal at the park is a dog or a cat\n",
    "\n",
    "It draws a hyperplane at the extreme data points aka Support vectors. SMV is a frontier of which best segregates the two classes(HYPER-PLANE/LINE).\n",
    "\n",
    "Using the optimal separation/hyper plane is important because unoptimized decision boundary could result in greater miscalculations on new data.\n",
    "\n",
    "Support Vectors are data points that the margins push against or points that are closes to the opposing class. So the algorithms implies that only support vectors are important whereas other training examples are ignorable.\n",
    "\n",
    "So we have \n",
    "1. D+ which is the shortest distance to the closest positive point, and \n",
    "2. D-ve which is the shortest distance to the closest negative point, and\n",
    "3. Margin of a separator hyper-plane which is \n",
    "         Margin = D+ + D-ve\n",
    "\n",
    "The line or boundary that separates the 2 classes is commonly referred to as a Hyper-plane because SVMs can be used for multiple applications and the data points are referred to as vectors.\n",
    "\n",
    "Types of SVM\n",
    "1. Linear Support Vector Machine (LSVM) - the classes are linearly separable\n",
    "When we have Non-linear SVM i.e. when it's impossible to use a straight line to separate the classes because it will go through data points for both classes, we can use a function to project our data into high dimensional space i.e. 1D to 2D or 2D to 3D and so on.\n",
    "\n",
    "The only thing with projecting to a higer dimension is that it is computationaly expensive. We can use a __kernel trick__ to reduce the computational cost. A function that takes as its inputs vectors in the original space and returns the dot product of the vectors in the feature space is called a __kernel function__, also referred to as a kernel trick.\n",
    "\n",
    "Using a kernel function, we can apply the dot product between 2 vectors so that every point is mapped into a high dimentional space via some transformation. i.e. we use it to transform a non linear space into a linear space.\n",
    "\n",
    "#### Popular Kernel Types\n",
    "which we can use to transform our data into higher dimensional feature space\n",
    "- Polynomial kernel\n",
    "- Radian basis function RBF kernel\n",
    "- Sigmoid Kernel \n",
    "- liner etc.\n",
    "\n",
    "Deciding on the kernel type  to use a non trivial task because no matter the kernel type you use, you need to __tune the kernel parameters__ to get good performance from a classifier. \n",
    "\n",
    "When solving a linear problem, using kernel='linear' when initializing the classifier could improve the accuracy score.\n",
    "\n",
    "### Popular Parameter Tuning techniques\n",
    "1. K-fold\n",
    "2. Cross validation\n",
    "\n",
    "\n",
    "#### Advantages of SVM\n",
    "- They are effective in high dimension spaces.\n",
    "- They are still effective where the number of dimensionns are greater than the number of samples.\n",
    "- They use a subset of training points in the decision function or support vectors so it's also memory efficient.\n",
    "- They are versatile so different kernels can be specified for the decision function.\n",
    "- Common kernels are provided but it's also possible to specify custom kernels.\n",
    "- We can add kernel functions together to achieve even more complex hyper planes.\n",
    "\n",
    "#### Disadvantages\n",
    "- If the number of features is greater than the number of samples, the method is likely to give poor performance.\n",
    "- SVMs do not directly provide probability estimates. These are calculated using expensive 5 fold cross validation (K-fold cross validation).\n",
    "\n",
    "#### Applications\n",
    "It has numerous applications and can be quite popular with neural networks e.g. medical imaging, air quality, image interpolation, time-series predictions, machine fault diagnosis, web page ranking algorithm etc.\n",
    "\n",
    "\n",
    "When modeling with SVM, the default kernal in sklearn is 'rbf'. You can specify a different kernel when initializing the Support Vector Classifier i.e. svc_model = SVC(kernel = 'newkernerlyouwanttouse')\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    svc_model = SVC()  # initilize the support vector classifier\n",
    "    svc_model.fit(X_train, y_train)  # model\n",
    "    y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "    \n",
    "    y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "    \n",
    "    # check the accuracy\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print('Testing data accuracy score')\n",
    "    print(metrics.accuracy_score(y_test, y_pred)\n",
    "    print('=============================')\n",
    "    print('Training data accuracy score')\n",
    "    print(metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM example\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC()  # initilize the support vector classifier\n",
    "svc_model.fit(X_train, y_train)  # model\n",
    "y_pred = svc_model.predict(X_test)  # predict using test data\n",
    "\n",
    "y_train_pred = svc_model.predict(X_train)  # predict using train data\n",
    "\n",
    "# check the accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Testing data accuracy score')\n",
    "print(metrics.accuracy_score(y_test, y_pred)\n",
    "print('=============================')\n",
    "print('Training data accuracy score')\n",
    "print(metrics.accuracy_score(y_train, y_train_pred))\n",
    "      \n",
    "      \n",
    "# to view the boundary - first create a mesh i.e. the axis tick/grid values\n",
    "X = X = iris.data[:, :2]    # matplotlib is for 2D maps so you need to specify [:, :2] where :2 will select 'Sepal Length' and 'Sepal width'\n",
    "y = iris.target\n",
    "      \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = (x_max / x_min)/100\n",
    "xx, yy = np.meshgrid(np.arrange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "                     \n",
    "plt.subplot(1, 1, 1)\n",
    "Z = svc_iris_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpa=0.8)  # to show the boundary/plane of separation cmap=plt.cm.Paired means to make the data points same color as the plane colors\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.title('SVC with linear kernel')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "- It is a technique used for improving prediction accuracy\n",
    "- It uses multpl learning lgorithms instead of simgle algorithm\n",
    "\n",
    "You use multiple homogenous algorithms i.e multiple weak learners are combined to form a strong learner i.e. more generalized rather than overfittef.\n",
    "\n",
    "2 Commonly used ensemble methods are:\n",
    "- Bagging/Boostrap aggregating \n",
    "- Boosting\n",
    "\n",
    "Bagging is a machine learning ensemble meta-algorithm designed to improve the stability, reduce variance, and accuracy.\n",
    "\n",
    "Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning. Algorithms that achieve hypothesis boosting simply become known as boosting.\n",
    "\n",
    "\n",
    "### Bagging\n",
    "Example of Bagging is __Random Forest Classifier__ (n nnumber of decision trees put together). Random Forest is a bagging technique based on decision tree algorithm. The input data to the algorithm is sub sampled (with __replacement__) and sent to every decision tree initialized earlier at a __random__ fashion.\n",
    "\n",
    "It grows multiple decision trees (as opposed to a single tree) using algorithms such as:\n",
    "- information gain\n",
    "- Gini index approach\n",
    "- other decision tree algorithms\n",
    "\n",
    "Each tree gives a classification and we say that the tree votes for that class. The classification with the most votes is chosen. In the case of regression, it takes the average of the outputs by different trees.\n",
    "\n",
    "# Random Forest Classfier\n",
    "#### Advantages\n",
    "- Can be used for both classification and regression tasks. \n",
    "- Will handle the missing values and maintains accuracy when a large proportion of the data are missing data\n",
    "- Won't overfit the model \n",
    "- Handle large dataset with higher dimensionality\n",
    "\n",
    "#### Disadvantages\n",
    "- Good job at classification but not as good for regression problems (as it does not give continuous nature of predictions)\n",
    "- YOu have very little control on what the model does. It can seem like a black box to statisticians. You can try different parameters and random seeds.\n",
    "\n",
    "- Banking - identify Loyal cxs and fraudulent cxs\n",
    "- Medicine - identiy disease by analyzing the patients medical records\n",
    "- Identify stock behavior and expected loss/profit by buying a particular stock\n",
    "- Likelihood of a customer liking the recommened product\n",
    "- Image classification\n",
    "- Lip reading and Voice classification\n",
    "\n",
    "#### Random Forest Pseudocode\n",
    "1. Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replcement.\n",
    "\n",
    "2. If there are M input variable or features, a number m<M is specified such that at each node, m variables are selected at random our of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
    "\n",
    "3. Each tree is grown to the largest ectent possible and there is no pruning\n",
    "\n",
    "4. Predict new data by aggregating the predictions og the n tree trees (i.e. majority votes for classification, average for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
