{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.22 Text Mining - Structuring Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Structure\n",
    "\n",
    "    - S = NP + VP i.e. Sentence is formed when Noun Phrase is combined with Verb Phrase\n",
    "    - NP = N + Determiner(optional) i.e. Noun is combined with a determiner, which is optional\n",
    "    - VP = V(NP) + (PP) i.e Verb is combined optionally with a Noun Phrase and Prepositional phase\n",
    "    - PP = P + NP i.e. preposition and noun phrase\n",
    "\n",
    "### Chunking\n",
    "    - PRP Preposition e.g. We\n",
    "    - VBD Verb e.g. saw\n",
    "    - DT Determiner e.g. the\n",
    "    - JJ Adjective e.g. yellow\n",
    "    - NN Noun e.g. dog\n",
    "\n",
    "### Chinking\n",
    "\n",
    "### Problem Statement\n",
    "A company wants to perform text analysis for one of its datasets.\n",
    "You are provided with the dataset named 'Tweets.csv' which has tweets od 6 US airlines along with their sentiments: positive, negative, and neutral. The tweets are present in the 'text' column and sentiments in 'airline_sentiment' column. We will be retrieving all tags starting with the rate (@) in the dataset and save the output in a file called 'References.txt'\n",
    "\n",
    "Import the pandas library and read the tweet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pathtogs = 'C:\\\\Program Files\\\\gs\\\\gs9.24\\\\bin'\n",
    "\n",
    "os.environ['PATH']+=os.pathsep+pathtogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "df = df[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the dataset with regex to find the relevant tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r = re.compile(r'([@])(\\w+)\\b')\n",
    "\n",
    "AllReferences = map(lambda x: r.findall(x), df['text'])\n",
    "\n",
    "import itertools\n",
    "AllUniqueReferencesCombined = set(list(itertools.chain.from_iterable(AllReferences)))\n",
    "References = map(lambda x:x[0]+x[1], AllUniqueReferencesCombined)\n",
    "\n",
    "# the result is stored in a file named references.txt\n",
    "file = open('References.txt','a')\n",
    "for each in References:\n",
    "    file.write(each +'\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all noun phrase and save them in a file named Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def GetNounPhrases(s):\n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(s)\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    except:\n",
    "        return[]\n",
    "    \n",
    "    else: \n",
    "        grammar = r\"NP:{<DT><NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS>}\"\n",
    "        # 3 sentiment values +ve, -ve, and neutral\n",
    "            \n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        \n",
    "        # iterate the leaf nodes and assign them to noun_phrase variable\n",
    "        noun_phrases_list = [[''.join(leaf[0] for leaf in tree.leaves())\n",
    "                            for tree in cp.parse(sent).subtrees()\n",
    "                            if tree.label()=='NP']\n",
    "                            for sent in sentences]\n",
    "        return noun_phrases_list\n",
    "        \n",
    "import itertools    \n",
    "for group,sub in df.groupby('airline_sentiment'):\n",
    "    noun_phrases = map(lambda x: GetNounPhrases(x), sub['text'])   # get all the noun phrases\n",
    "    noun_phrases = list(itertools.chain.from_iterable(noun_phrases))   # putting it into list\n",
    "    AllNounPhrases = set(list(itertools.chain.from_iterable(noun_phrases)))\n",
    "    # create a filename \n",
    "    filename = 'Noun Phrases for '+ str(group) + 'Review.txt'\n",
    "    file = open(filename,'a', encoding=\"utf-8\")\n",
    "    for each in AllNounPhrases:\n",
    "        file.write(each+'\\n')\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was done in above step:\n",
    "\n",
    "We had 3 sentiment values: positive, negative, and neutral  \n",
    "    \n",
    "    grammar = r'NP:{<DT><NN|NNS|NNP|NNPS>*<NN|NNS|NNP|NNPS}' \n",
    "\n",
    "We iterated through all the leaf nodes and assigned them to noun_phrases_list variable\n",
    "    \n",
    "This means that the functions in itertools operate on iterators to produce more complex iterators\n",
    "\n",
    "Using the map function, we got all the noun phrases from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
